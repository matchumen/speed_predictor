{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, 'viroco/src')\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import folium\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/preprocessed'\n",
    "drive_pths = [os.path.join(root, fn) for fn in os.listdir(root)]\n",
    "feature_cols = ['target_speed', 'speed_osrm', 'way_maxspeed', 'elevation', 'fwd_azimuth', 'way_type', 'way_surface', 'node:intersection', 'node:railway', 'node:crossing', 'node:highway', 'node:stop', 'start_stop', 'azimuth_diff', 'elevation_diff']\n",
    "\n",
    "drives = []\n",
    "for df_pth in drive_pths[:]:\n",
    "    df = pd.read_csv(df_pth, index_col='index')\n",
    "    df = df[feature_cols]\n",
    "    drives.append(df)\n",
    "    \n",
    "df = pd.concat(drives)\n",
    "del(drives)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/valid_categories.json', 'r') as f:\n",
    "    valid_categories = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.way_maxspeed = df.way_maxspeed / 3.6 # unify units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = valid_categories.keys()\n",
    "for cc in categorical_cols:\n",
    "    df[cc].fillna('null', inplace=True) # fill NaN values with 'null' string\n",
    "    df.loc[~df[cc].isin(valid_categories[cc]), cc] = 'null' # set values not in valid_categories to 'null' string\n",
    "\n",
    "# encode categorical columns to one-hot    \n",
    "encoded_categoricals = pd.concat([pd.get_dummies(df[cc], prefix=cc) for cc in categorical_cols], axis=1)\n",
    "df.drop(columns=categorical_cols, inplace=True)\n",
    "df = pd.concat([df, encoded_categoricals], axis=1)\n",
    "del(encoded_categoricals)\n",
    "\n",
    "window_size = 201\n",
    "pad_size = window_size // 2\n",
    "\n",
    "for cc in categorical_cols:\n",
    "    for val in valid_categories[cc]:\n",
    "        col_name = str(cc) + '_' + str(val)\n",
    "        # ensure all columns\n",
    "        if col_name not in df:\n",
    "            df[col_name] = 0\n",
    "            \n",
    "        if col_name.startswith('node:') or col_name.startswith('start_stop'):\n",
    "            padded = np.pad(df[col_name], (pad_size), 'constant', constant_values=(0))\n",
    "            padded_series = pd.Series(padded).rolling(window_size, win_type='triang', center=True).sum().dropna()\n",
    "            padded_series.reset_index(inplace=True, drop=True)\n",
    "            df[col_name] = padded_series\n",
    "        \n",
    "# drop complementary columns\n",
    "df.drop(columns=[cc for cc in df.columns.to_list() if cc.endswith('_null')] + ['start_stop_0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns in DataFrame\n",
    "ordered_columns = sorted(df.columns.to_list()[1:])\n",
    "ordered_columns.insert(0, 'target_speed')\n",
    "df = df[ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, scaler_fn=None):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    df = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    \n",
    "    if scaler_fn:\n",
    "        with open(scaler_fn, 'wb') as out:\n",
    "            pickle.dump(scaler, out)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = min_max_scale(df, 'min_max_scaler')\n",
    "\n",
    "df.to_csv('test.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('candid': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd0d2c1020507c5971c978df260573accb7ca1864edff9b410d49e4df6675fcfe5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
